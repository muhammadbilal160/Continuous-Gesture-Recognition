{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at C:\\Users\\Bilal\\.conda\\envs\\tf:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_tflow_select             2.2.0                     eigen  \n",
      "absl-py                   0.9.0                    py37_0  \n",
      "argon2-cffi               20.1.0           py37h4ab8f01_1    conda-forge\n",
      "astor                     0.8.1                    py37_0  \n",
      "attrs                     19.3.0                     py_0    conda-forge\n",
      "backcall                  0.2.0              pyh9f0ad1d_0    conda-forge\n",
      "backports                 1.0                        py_2    conda-forge\n",
      "backports.functools_lru_cache 1.6.1                      py_0    conda-forge\n",
      "blas                      1.0                         mkl  \n",
      "bleach                    3.1.5              pyh9f0ad1d_0    conda-forge\n",
      "blinker                   1.4                      py37_0  \n",
      "brotlipy                  0.7.0           py37he774522_1000  \n",
      "ca-certificates           2020.6.20            hecda079_0    conda-forge\n",
      "cachetools                4.1.1                      py_0  \n",
      "certifi                   2020.6.20        py37hc8dfbb8_0    conda-forge\n",
      "cffi                      1.14.0           py37h7a1dbc1_0  \n",
      "chardet                   3.0.4                 py37_1003  \n",
      "click                     7.1.2                      py_0  \n",
      "colorama                  0.4.3                      py_0    conda-forge\n",
      "cryptography              2.9.2            py37h7a1dbc1_0  \n",
      "cycler                    0.10.0                     py_2    conda-forge\n",
      "decorator                 4.4.2                      py_0    conda-forge\n",
      "defusedxml                0.6.0                      py_0    conda-forge\n",
      "entrypoints               0.3             py37hc8dfbb8_1001    conda-forge\n",
      "freetype                  2.10.2               hd328e21_0    conda-forge\n",
      "gast                      0.2.2                    py37_0  \n",
      "google-auth               1.20.1                     py_0  \n",
      "google-auth-oauthlib      0.4.1                      py_2  \n",
      "google-pasta              0.2.0                      py_0  \n",
      "grpcio                    1.27.2           py37h351948d_0  \n",
      "h5py                      2.10.0           py37h5e291fa_0  \n",
      "hdf5                      1.10.4               h7ebc959_0  \n",
      "icc_rt                    2019.0.0             h0cc432a_1  \n",
      "icu                       64.2                 he025d50_1    conda-forge\n",
      "idna                      2.10                       py_0  \n",
      "importlib-metadata        1.7.0                    py37_0  \n",
      "importlib_metadata        1.7.0                         0    conda-forge\n",
      "intel-openmp              2020.1                      216  \n",
      "ipykernel                 5.3.4            py37h5ca1d4c_0    conda-forge\n",
      "ipython                   7.17.0           py37hc6149b9_0    conda-forge\n",
      "ipython_genutils          0.2.0                      py_1    conda-forge\n",
      "jedi                      0.15.2                   py37_0    conda-forge\n",
      "jinja2                    2.11.2             pyh9f0ad1d_0    conda-forge\n",
      "joblib                    0.16.0                     py_0    conda-forge\n",
      "jpeg                      9d                   he774522_0    conda-forge\n",
      "json5                     0.9.4              pyh9f0ad1d_0    conda-forge\n",
      "jsonschema                3.2.0            py37hc8dfbb8_1    conda-forge\n",
      "jupyter_client            6.1.6                      py_0    conda-forge\n",
      "jupyter_core              4.6.3            py37hc8dfbb8_1    conda-forge\n",
      "jupyterlab                2.2.4                      py_0    conda-forge\n",
      "jupyterlab_server         1.2.0                      py_0    conda-forge\n",
      "keras-applications        1.0.8                      py_1  \n",
      "keras-preprocessing       1.1.0                      py_1  \n",
      "kiwisolver                1.2.0            py37heaa310e_0    conda-forge\n",
      "libblas                   3.8.0                    16_mkl    conda-forge\n",
      "libcblas                  3.8.0                    16_mkl    conda-forge\n",
      "libclang                  9.0.1           default_hf44288c_0    conda-forge\n",
      "liblapack                 3.8.0                    16_mkl    conda-forge\n",
      "liblapacke                3.8.0                    16_mkl    conda-forge\n",
      "libopencv                 4.4.0                    py37_2    conda-forge\n",
      "libpng                    1.6.37               ha81a0f5_2    conda-forge\n",
      "libprotobuf               3.12.4               h200bbdf_0  \n",
      "libsodium                 1.0.17               h2fa13f4_0    conda-forge\n",
      "libtiff                   4.1.0                h885aae3_6    conda-forge\n",
      "libwebp-base              1.1.0                hfa6e2cd_3    conda-forge\n",
      "lz4-c                     1.9.2                h62dcd97_1    conda-forge\n",
      "m2w64-gcc-libgfortran     5.3.0                         6  \n",
      "m2w64-gcc-libs            5.3.0                         7  \n",
      "m2w64-gcc-libs-core       5.3.0                         7  \n",
      "m2w64-gmp                 6.1.0                         2  \n",
      "m2w64-libwinpthread-git   5.0.0.4634.697f757               2  \n",
      "markdown                  3.2.2                    py37_0  \n",
      "markupsafe                1.1.1            py37h8055547_1    conda-forge\n",
      "matplotlib                3.3.1                         0    conda-forge\n",
      "matplotlib-base           3.3.1            py37h35e8a6e_0    conda-forge\n",
      "mistune                   0.8.4           py37h8055547_1001    conda-forge\n",
      "mkl                       2020.1                      216  \n",
      "mkl-service               2.3.0            py37hb782905_0  \n",
      "mkl_fft                   1.1.0            py37h45dec08_0  \n",
      "mkl_random                1.1.1            py37h47e9c7a_0  \n",
      "msys2-conda-epoch         20160418                      1  \n",
      "nbconvert                 5.6.1            py37hc8dfbb8_1    conda-forge\n",
      "nbformat                  5.0.7                      py_0    conda-forge\n",
      "notebook                  6.1.3            py37hc8dfbb8_0    conda-forge\n",
      "numpy                     1.19.1           py37h5510c5b_0  \n",
      "numpy-base                1.19.1           py37ha3acd2a_0  \n",
      "oauthlib                  3.1.0                      py_0  \n",
      "olefile                   0.46                       py_0    conda-forge\n",
      "opencv                    4.4.0                    py37_2    conda-forge\n",
      "openssl                   1.1.1g               he774522_1    conda-forge\n",
      "opt_einsum                3.1.0                      py_0  \n",
      "packaging                 20.4               pyh9f0ad1d_0    conda-forge\n",
      "pandoc                    2.10.1               he774522_0    conda-forge\n",
      "pandocfilters             1.4.2                      py_1    conda-forge\n",
      "parso                     0.8.0              pyh9f0ad1d_0    conda-forge\n",
      "pickleshare               0.7.5           py37hc8dfbb8_1001    conda-forge\n",
      "pillow                    7.2.0            py37hc826c6e_1    conda-forge\n",
      "pip                       20.2.2                   py37_0  \n",
      "prometheus_client         0.8.0              pyh9f0ad1d_0    conda-forge\n",
      "prompt-toolkit            3.0.6                      py_0    conda-forge\n",
      "protobuf                  3.12.4           py37ha925a31_0  \n",
      "py-opencv                 4.4.0            py37h43977f1_2    conda-forge\n",
      "pyasn1                    0.4.8                      py_0  \n",
      "pyasn1-modules            0.2.7                      py_0  \n",
      "pycparser                 2.20                       py_2  \n",
      "pygments                  2.6.1                      py_0    conda-forge\n",
      "pyjwt                     1.7.1                    py37_0  \n",
      "pyopenssl                 19.1.0                     py_1  \n",
      "pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge\n",
      "pyqt                      5.12.3           py37h6538335_1    conda-forge\n",
      "pyqt5-sip                 4.19.18                  pypi_0    pypi\n",
      "pyqtwebengine             5.12.1                   pypi_0    pypi\n",
      "pyreadline                2.1                      py37_1  \n",
      "pyrsistent                0.16.0           py37h8055547_0    conda-forge\n",
      "pysocks                   1.7.1                    py37_1  \n",
      "python                    3.7.7                h81c818b_4  \n",
      "python-dateutil           2.8.1                      py_0    conda-forge\n",
      "python_abi                3.7                     1_cp37m    conda-forge\n",
      "pywin32                   227              py37hfa6e2cd_0    conda-forge\n",
      "pywinpty                  0.5.7                    py37_0    conda-forge\n",
      "pyzmq                     19.0.2           py37h453f00a_0    conda-forge\n",
      "qt                        5.12.5               h7ef1ec2_0    conda-forge\n",
      "requests                  2.24.0                     py_0  \n",
      "requests-oauthlib         1.3.0                      py_0  \n",
      "rsa                       4.6                        py_0  \n",
      "scikit-learn              0.23.2           py37hdc70db3_0    conda-forge\n",
      "scipy                     1.5.0            py37h9439919_0  \n",
      "send2trash                1.5.0                      py_0    conda-forge\n",
      "setuptools                49.6.0                   py37_0  \n",
      "six                       1.15.0                     py_0  \n",
      "sqlite                    3.32.3               h2a8f88b_0  \n",
      "tensorboard               2.2.1              pyh532a8cf_0  \n",
      "tensorboard-plugin-wit    1.6.0                      py_0  \n",
      "tensorflow                2.1.0           eigen_py37hd727fc0_0  \n",
      "tensorflow-base           2.1.0           eigen_py37h49b2757_0  \n",
      "tensorflow-estimator      2.1.0              pyhd54b08b_0  \n",
      "termcolor                 1.1.0                    py37_1  \n",
      "terminado                 0.8.3            py37hc8dfbb8_1    conda-forge\n",
      "testpath                  0.4.4                      py_0    conda-forge\n",
      "threadpoolctl             2.1.0              pyh5ca1d4c_0    conda-forge\n",
      "tk                        8.6.10               he774522_0    conda-forge\n",
      "tornado                   6.0.4            py37hfa6e2cd_0    conda-forge\n",
      "traitlets                 4.3.3            py37hc8dfbb8_1    conda-forge\n",
      "urllib3                   1.25.10                    py_0  \n",
      "vc                        14.1                 h0510ff6_4  \n",
      "vs2015_runtime            14.16.27012          hf0eaf9b_3  \n",
      "wcwidth                   0.2.5              pyh9f0ad1d_1    conda-forge\n",
      "webencodings              0.5.1                      py_1    conda-forge\n",
      "werkzeug                  0.16.1                     py_0  \n",
      "wheel                     0.34.2                   py37_0  \n",
      "win_inet_pton             1.1.0                    py37_0  \n",
      "wincertstore              0.2                      py37_0  \n",
      "winpty                    0.4.3                         4    conda-forge\n",
      "wrapt                     1.12.1           py37he774522_1  \n",
      "xz                        5.2.5                h62dcd97_1    conda-forge\n",
      "zeromq                    4.3.2                ha925a31_3    conda-forge\n",
      "zipp                      3.1.0                      py_0  \n",
      "zlib                      1.2.11               h62dcd97_4  \n",
      "zstd                      1.4.5                h1f3a1b7_2    conda-forge\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 160867,
     "status": "ok",
     "timestamp": 1597398983526,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     },
     "user_tz": -300
    },
    "id": "WnlDjyrVCnrm",
    "outputId": "c6499fdb-0e12-4b15-d925-fd7e98c060a0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense,MaxPool2D,Dropout,Flatten,Conv2D,GlobalAveragePooling2D,Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from random import choice,shuffle\n",
    "from scipy import stats as st\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 817,
     "status": "ok",
     "timestamp": 1597398993676,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     },
     "user_tz": -300
    },
    "id": "iojr_NWQsGLw"
   },
   "outputs": [],
   "source": [
    "#os.chdir('/content/drive/My Drive/Colab Notebooks/Gesture Recognition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stBnYNlhfdhG"
   },
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1597387928333,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     },
     "user_tz": -300
    },
    "id": "2qwRjnL6Cnrv"
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "def gather_data(num_samples):\n",
    "    \n",
    "    global ThumbUp, ThumbDown, StopSign, Point, Pick, Nogestures\n",
    "    \n",
    "    \n",
    "    # Initialize the camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # trigger tells us when to start recording\n",
    "    trigger = False\n",
    "    \n",
    "    # Counter keeps count of the number of samples collected\n",
    "    counter = 0\n",
    "    \n",
    "    # This the ROI size, the size of images saved will be box_size -10\n",
    "    box_size = 234\n",
    "    \n",
    "    # Getting the width of the frame from the camera properties\n",
    "    width = int(cap.get(3))\n",
    "\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        # Read frame by frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Flip the frame laterally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Break the loop if there is trouble reading the frame.\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # If counter is equal to the number samples then reset triger and the counter\n",
    "        if counter == num_samples:\n",
    "            trigger = not trigger\n",
    "            counter = 0\n",
    "        \n",
    "        # Define ROI for capturing samples\n",
    "        cv2.rectangle(frame, (width - box_size, 0), (width, box_size), (0, 250, 150), 2)\n",
    "        \n",
    "        # Make a resizable window.\n",
    "        cv2.namedWindow(\"Collecting images\", cv2.WINDOW_NORMAL)\n",
    "        \n",
    "        \n",
    "        # If trigger is True than start capturing the samples\n",
    "        if trigger:\n",
    "            \n",
    "            # Grab only slected roi\n",
    "            roi = frame[5: box_size-5 , width-box_size + 5: width -5]\n",
    "            \n",
    "            # Append the roi and class name to the list with the selected class_name\n",
    "            eval(class_name).append([roi, class_name])\n",
    "                                    \n",
    "            # Increment the counter \n",
    "            counter += 1 \n",
    "        \n",
    "            # Text for the counter\n",
    "            text = \"Collected Samples of {}: {}\".format(class_name, counter)\n",
    "            \n",
    "        else:\n",
    "            text = \"Press 'u' to collect Thumb Up samples, 'd' for Thumb Down, 's' for Stop Sign,'p' for Point', 'i' for Pick and 'n' for No gestures\"\n",
    "        \n",
    "        # Show the counter on the imaege\n",
    "        cv2.putText(frame, text, (3, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # Display the window\n",
    "        cv2.imshow(\"Collecting images\", frame)\n",
    "        \n",
    "        # Wait 1 ms\n",
    "        k = cv2.waitKey(1)\n",
    "        \n",
    "        # If user press 'u' then class_name is set to ThumbUp and trigger set to True  \n",
    "        if k == ord('u'):\n",
    "            \n",
    "            # Trigger the variable inorder to capture the samples\n",
    "            trigger = not trigger\n",
    "            class_name = 'ThumbUp'\n",
    "            ThumbUp = []\n",
    "           \n",
    "            \n",
    "        # If user press 'd' then class_name is set to ThumbDown and trigger set to True  \n",
    "        if k == ord('d'):\n",
    "            trigger = not trigger\n",
    "            class_name = 'ThumbDown'\n",
    "            ThumbDown = []\n",
    "        \n",
    "        # If user press 'p' then class_name is set to Point and trigger set to True  \n",
    "        if k == ord('p'):\n",
    "            trigger = not trigger\n",
    "            class_name = 'Point'\n",
    "            Point = []\n",
    "                    \n",
    "        # If user press 'n' then class_name is set to Nogestures and trigger set to True\n",
    "        if k == ord('n'):\n",
    "            trigger = not trigger\n",
    "            class_name = 'Nogestures'\n",
    "            Nogestures = []\n",
    "            \n",
    "        # If user press 'i' then class_name is set to Pick and trigger set to True\n",
    "        if k == ord('i'):\n",
    "            trigger = not trigger\n",
    "            class_name = 'Pick'\n",
    "            Pick = []\n",
    "\n",
    "       # If user press 'z' then class_name is set to nothing and trigger set to True\n",
    "        if k == ord('s'):\n",
    "            trigger = not trigger\n",
    "            class_name = 'StopSign'\n",
    "            StopSign = []\n",
    "\n",
    "            \n",
    "        # Exit if user presses 'q'\n",
    "        if k == ord('q'):\n",
    "            break\n",
    "            \n",
    "    #  Release the camera and destroy the window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1297,
     "status": "ok",
     "timestamp": 1597387934975,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     },
     "user_tz": -300
    },
    "id": "ls1tJPbiCnr3"
   },
   "outputs": [],
   "source": [
    "#run this when you want to create your own dataset\n",
    "no_of_samples = 400\n",
    "gather_data(no_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eqgbZ9T8ft_0"
   },
   "outputs": [],
   "source": [
    "# Save dataset on disk so that it can be used again later.\n",
    "import pickle\n",
    "\n",
    "for gesture in ['ThumbUp', 'ThumbDown', 'StopSign', 'Point', 'Pick', 'Nogestures']:\n",
    "    with open(f'{gesture}.pickle', 'wb') as f:\n",
    "        pickle.dump(eval(gesture), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKzaGLSvfab5"
   },
   "source": [
    "We are keeping our entire dataset in memory during training so a large amount of ram is required for training. If training gives a MemoryError on your device, use Google Colab. Upload this notebook as well as the pickle files containing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7637,
     "status": "ok",
     "timestamp": 1597399007572,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     },
     "user_tz": -300
    },
    "id": "aLk3gy-oCnr_",
    "outputId": "07bb7d86-f2da-4791-d771-2e1938a17c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThumbUp\n",
      "ThumbDown\n",
      "StopSign\n",
      "Point\n",
      "Pick\n",
      "Nogestures\n"
     ]
    }
   ],
   "source": [
    "# Only run this cell if you are training on Google Colab or on a different device than the one you collected the dataset on.\n",
    "import pickle\n",
    "\n",
    "temp = []\n",
    "for gesture in ['ThumbUp', 'ThumbDown', 'StopSign', 'Point', 'Pick', 'Nogestures']:\n",
    "  print(gesture)\n",
    "  temp.append(pickle.load( open( f'{gesture}.pickle', \"rb\" ) ) )\n",
    "\n",
    "ThumbUp, ThumbDown, StopSign, Point, Pick, Nogestures = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4117,
     "status": "ok",
     "timestamp": 1597399068187,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     },
     "user_tz": -300
    },
    "id": "lFMP7j-JCnsE",
    "outputId": "91c42bb7-0339-4ee0-ee8a-736ce78a84e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 2400 , Total Labels: 2400\n"
     ]
    }
   ],
   "source": [
    "# Combine the labels of all classes together\n",
    "labels = [tupl[1] for tupl in ThumbUp] + [tupl[1] for tupl in ThumbDown] + [tupl[1] for tupl in StopSign] + [tupl[1] for tupl in Point] + [tupl[1] for tupl in Pick]+[tupl[1] for tupl in Nogestures]\n",
    "\n",
    "# Combine the images of all classes together\n",
    "images = [tupl[0] for tupl in ThumbUp] + [tupl[0] for tupl in ThumbDown] + [tupl[0] for tupl in StopSign] + [tupl[0] for tupl in Point]+[tupl[0] for tupl in Pick] + [tupl[0] for tupl in Nogestures]\n",
    "\n",
    "# Normalize the images by dividing by 255, now our images are in range 0-1. This will help in training.\n",
    "images = np.array(images, dtype=\"float\") / 255.0\n",
    "\n",
    "# Print out the total number of labels and images.\n",
    "print('Total images: {} , Total Labels: {}'.format(len(labels), len(images)))\n",
    "\n",
    "# Create an encoder Object\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Convert Lablels to integers. mapping is done in alphabatical order\n",
    "Int_labels = encoder.fit_transform(labels)\n",
    "\n",
    "# Now the convert the integer labels into one hot format. i.e. 0 = [1,0,0,0]  etc.\n",
    "one_hot_labels = to_categorical(Int_labels, 6)\n",
    "\n",
    "# Now we're splitting the data, 75% for training and 25% for testing.\n",
    "(trainX, testX, trainY, testY) = train_test_split(images, one_hot_labels, test_size=0.25, random_state=50)\n",
    "\n",
    "# Empty memory from RAM\n",
    "images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15500,
     "status": "ok",
     "timestamp": 1597399134171,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     },
     "user_tz": -300
    },
    "id": "Dfh3BkiNCnsL",
    "outputId": "b7b239bb-9a13-4887-b502-adce0e11e0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-mobile-no-top.h5\n",
      "19996672/19993432 [==============================] - 174s 9us/step\n",
      "Number of Layers in Model: 773\n"
     ]
    }
   ],
   "source": [
    "# This is the input size which our model accepts.\n",
    "image_size = 224\n",
    "\n",
    "# Loading pre-trained NASNETMobile Model without the head by doing include_top = False\n",
    "N_mobile = tf.keras.applications.NASNetMobile( input_shape=(image_size, image_size, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Freeze the whole model \n",
    "N_mobile.trainable = False\n",
    "    \n",
    "# Adding our own custom head\n",
    "# Start by taking the output feature maps from NASNETMobile\n",
    "x = N_mobile.output\n",
    "\n",
    "# Convert to a single-dimensional vector by Global Average Pooling. \n",
    "# We could also use Flatten()(x) GAP is more effective reduces params and controls overfitting.\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Adding a dense layer with 712 units\n",
    "x = Dense(712, activation='relu')(x) \n",
    "\n",
    "# Dropout 40% of the activations, helps reduces overfitting\n",
    "x = Dropout(0.40)(x)\n",
    "\n",
    "# The fianl layer will contain 6 output units (no of units = no of classes) with softmax function.\n",
    "preds = Dense(6,activation='softmax')(x)\n",
    "\n",
    "# Construct the full model\n",
    "model = Model(inputs=N_mobile.input, outputs=preds)\n",
    "\n",
    "# Check the number of layers in the final Model\n",
    "print (\"Number of Layers in Model: {}\".format(len(model.layers[:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5066,
     "status": "ok",
     "timestamp": 1597399134177,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     },
     "user_tz": -300
    },
    "id": "qP5RAfBCCnsR"
   },
   "outputs": [],
   "source": [
    "# Data augmentation in order to increase the size of our dataset and improve model performance.\n",
    "\n",
    "augment = ImageDataGenerator( \n",
    "        # rotation_range=30,\n",
    "        zoom_range=0.25,\n",
    "        width_shift_range=0.10,\n",
    "        height_shift_range=0.10,\n",
    "        shear_range=0.10,\n",
    "        horizontal_flip=False,\n",
    "        fill_mode=\"nearest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1843,
     "status": "ok",
     "timestamp": 1597399136037,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     },
     "user_tz": -300
    },
    "id": "9FSVGqS9CnsV"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4e91ee44838e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 297439,
     "status": "ok",
     "timestamp": 1597399432643,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     },
     "user_tz": -300
    },
    "id": "cLS4RjsACnsZ",
    "outputId": "34cc588e-78d2-4d7d-9cc9-6ce77a170196"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-390e4b48faf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Start training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m history = model.fit(x=augment.flow(trainX, trainY, batch_size=batchsize), \n\u001b[0m\u001b[0;32m      8\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainX' is not defined"
     ]
    }
   ],
   "source": [
    "# Set batchsize according to your system\n",
    "epochs = 10\n",
    "# batchsize = 5\n",
    "batchsize = 16\n",
    "\n",
    "# Start training\n",
    "history = model.fit(x=augment.flow(trainX, trainY, batch_size=batchsize), \n",
    "                    validation_data=(testX, testY), \n",
    "                    steps_per_epoch=(len(trainX) // batchsize), \n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1315,
     "status": "ok",
     "timestamp": 1597399475316,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     },
     "user_tz": -300
    },
    "id": "1d4FbmP_Cnse",
    "outputId": "53080162-aecc-4dd1-b00b-8b1ed0c5f871"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-03bd1c2ae0eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plot the accuracy and loss curves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the accuracy and loss curves\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2893,
     "status": "ok",
     "timestamp": 1597399553724,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     },
     "user_tz": -300
    },
    "id": "3kO4uQ8SCnsj"
   },
   "outputs": [],
   "source": [
    "#model.save(\"model.h5\")  # Saves model to h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bgLTxIfrhVtb"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1178db391e9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load saved model. IF this gives an error, make sure you have executed the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# cells above where the model is defined and compiled.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Load saved model. IF this gives an error, make sure you have executed the \n",
    "# cells above where the model is defined and compiled.\n",
    "model.load_weights('model.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "978BMuv2Cnsn"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8a8048f3f2f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# Get model's prediction.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Get the index of the target class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# This list will be used to map probabilities to class names, Label names are in alphabetical order.\n",
    "label_names = ['No gestures', 'Pick', 'Point', 'Stop Sign', 'Thumb Down', 'Thumb Up']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "box_size = 234\n",
    "width = int(cap.get(3))\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    frame = cv2.flip(frame, 1)\n",
    "           \n",
    "    cv2.rectangle(frame, (width - box_size, 0), (width, box_size), (0, 250, 150), 2)\n",
    "        \n",
    "    cv2.namedWindow(\"Gestures\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "    roi = frame[5: box_size-5 , width-box_size + 5: width -5]\n",
    "    \n",
    "    # Normalize the image like we did in the preprocessing step, also convert float64 array.\n",
    "    roi = np.array([roi]).astype('float64') / 255.0\n",
    " \n",
    "    # Get model's prediction.\n",
    "    pred = model.predict(roi)\n",
    "    \n",
    "    # Get the index of the target class.\n",
    "    target_index = np.argmax(pred[0])\n",
    "\n",
    "    # Get the probability of the target class\n",
    "    prob = np.max(pred[0])\n",
    "\n",
    "    # Show results\n",
    "    cv2.putText(frame, \"prediction: {} {:.2f}%\".format(label_names[np.argmax(pred[0])], prob*100 ),\n",
    "                (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.90, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow(\"Final Year Project\", frame)\n",
    "    \n",
    "   \n",
    "    k = cv2.waitKey(1)\n",
    "    if k == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Back-up.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
